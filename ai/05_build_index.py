import polars as pl
import numpy as np
import faiss
import os
import gc 
from tqdm import tqdm
import glob

# --- C·∫§U H√åNH ---
WINDOW_SIZE = 30      
PREDICT_HORIZON = 5   
BATCH_SIZE = 500      
INDEX_FILE = "pattern_search.index"
META_FILE = "pattern_metadata.parquet"

def build_index():
    print("üöÄ B·∫ÆT ƒê·∫¶U X√ÇY D·ª∞NG SEARCH ENGINE (BATCH PROCESSING - FIXED)...")
    
    # 1. Kh·ªüi t·∫°o FAISS Index
    index = faiss.IndexFlatL2(WINDOW_SIZE)
    
    # 2. ƒê·ªçc d·ªØ li·ªáu ngu·ªìn
    try:
        lf = pl.scan_parquet("dataset_ml_ready.parquet")
        all_symbols = lf.select("symbol").unique().collect().to_series().to_list()
        print(f"   -> T·ªïng s·ªë m√£ c·∫ßn x·ª≠ l√Ω: {len(all_symbols)}")
    except Exception as e:
        print(f"‚ùå L·ªói ƒë·ªçc file: {e}")
        return

    # X√≥a file c≈© n·∫øu c√≥ ƒë·ªÉ ch·∫°y l·∫°i t·ª´ ƒë·∫ßu
    if os.path.exists(META_FILE):
        os.remove(META_FILE)
    
    # X√≥a c√°c file part c≈© n·∫øu b·ªã crash gi·ªØa ch·ª´ng
    for f in glob.glob("pattern_metadata_part_*.parquet"):
        try: os.remove(f)
        except: pass
        
    total_vectors = 0
    
    # 3. V√íNG L·∫∂P X·ª¨ L√ù THEO BATCH
    symbol_chunks = [all_symbols[i:i + BATCH_SIZE] for i in range(0, len(all_symbols), BATCH_SIZE)]
    
    for chunk_idx, symbols_batch in enumerate(tqdm(symbol_chunks, desc="Processing Batches")):
        
        # Load batch v√†o RAM
        try:
            df_batch = lf.filter(pl.col("symbol").is_in(symbols_batch)).collect().sort(["symbol", "ts"])
        except Exception as e:
            print(f"Skipping batch {chunk_idx} due to load error: {e}")
            continue

        batch_vectors = []
        # Chu·∫©n b·ªã dict ch·ª©a list thu·∫ßn Python
        batch_metadata = {
            "symbol": [],
            "date": [],
            "future_return": []
        }
        
        dfs = df_batch.partition_by("symbol", as_dict=True)
        
        for sym, sub_df in dfs.items():
            closes = sub_df["close"].to_numpy()
            dates = sub_df["ts"].to_numpy()
            
            if len(closes) < WINDOW_SIZE + PREDICT_HORIZON:
                continue
                
            # Sliding Window
            from numpy.lib.stride_tricks import sliding_window_view
            windows = sliding_window_view(closes[:-PREDICT_HORIZON], window_shape=WINDOW_SIZE)
            
            future_prices = closes[WINDOW_SIZE + PREDICT_HORIZON - 1:]
            current_prices = closes[WINDOW_SIZE - 1 : -PREDICT_HORIZON]
            
            min_len = min(len(windows), len(future_prices))
            windows = windows[:min_len]
            future_ret = (future_prices[:min_len] / current_prices[:min_len]) - 1
            valid_dates = dates[WINDOW_SIZE - 1 : WINDOW_SIZE - 1 + min_len]
            
            # Z-Score Normalization
            means = np.mean(windows, axis=1, keepdims=True)
            stds = np.std(windows, axis=1, keepdims=True)
            norm_windows = (windows - means) / (stds + 1e-6)
            
            # Append Vector
            batch_vectors.append(norm_windows)
            
            # --- FIX QUAN TR·ªåNG: D√πng .tolist() ƒë·ªÉ chuy·ªÉn Numpy -> Python List chu·∫©n ---
            batch_metadata["symbol"].extend([str(sym)] * min_len) # √âp ki·ªÉu string
            batch_metadata["date"].extend(valid_dates.tolist())   # √âp ki·ªÉu datetime chu·∫©n
            batch_metadata["future_return"].extend(future_ret.tolist()) # √âp ki·ªÉu float chu·∫©n

        # L∆∞u Batch
        if batch_vectors:
            X_batch = np.concatenate(batch_vectors, axis=0).astype('float32')
            index.add(X_batch)
            total_vectors += X_batch.shape[0]
            
            # --- FIX QUAN TR·ªåNG: Khai b√°o Schema r√µ r√†ng cho Polars ---
            schema = {
                "symbol": pl.String,
                "date": pl.Datetime, # Ho·∫∑c pl.Date t√πy d·ªØ li·ªáu g·ªëc, pl.Datetime an to√†n h∆°n
                "future_return": pl.Float64
            }
            
            try:
                # T·∫°o DataFrame v·ªõi schema c·ª©ng ƒë·ªÉ tr√°nh l·ªói Object
                df_meta_chunk = pl.DataFrame(batch_metadata, schema=schema)
                
                # L∆∞u file part
                part_filename = f"pattern_metadata_part_{chunk_idx}.parquet"
                df_meta_chunk.write_parquet(part_filename)
            except Exception as e:
                print(f"‚ö†Ô∏è L·ªói l∆∞u metadata batch {chunk_idx}: {e}")

        # D·ªçn RAM
        del df_batch, dfs, batch_vectors, batch_metadata
        gc.collect()

    # 4. L∆∞u FAISS Index
    print(f"\nüíæ ƒêang l∆∞u Index ({total_vectors} vectors)...")
    faiss.write_index(index, INDEX_FILE)
    
    # 5. G·ªôp Metadata
    print("üíæ ƒêang g·ªôp Metadata...")
    all_parts = glob.glob("pattern_metadata_part_*.parquet")
    if all_parts:
        try:
            # ƒê·ªçc t·∫•t c·∫£ file part v√† l∆∞u th√†nh 1 file
            pl.read_parquet("pattern_metadata_part_*.parquet").write_parquet(META_FILE)
            
            # X√≥a file l·∫ª
            for f in all_parts:
                os.remove(f)
            print(f"‚úÖ HO√ÄN T·∫§T! ƒê√£ t·∫°o Search Engine t·∫°i: {os.getcwd()}")
        except Exception as e:
            print(f"‚ùå L·ªói khi g·ªôp file metadata: {e}")
            print("Tuy nhi√™n c√°c file part v·∫´n c√≤n ƒë√≥, b·∫°n c√≥ th·ªÉ load l·∫ª ƒë∆∞·ª£c.")
    else:
        print("‚ö†Ô∏è Kh√¥ng c√≥ d·ªØ li·ªáu metadata n√†o ƒë∆∞·ª£c t·∫°o ra.")

if __name__ == "__main__":
    build_index()